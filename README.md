# FQE_load_fit   #open FQE_pretrain
1. use pip import d3rlpy, scope_rl
2. mujoco: 2.3.3 mujoco-py:2.1.2.14  https://blog.guptanitish.com/guide-to-install-openais-mujoco-on-ubuntu-linux-1ac22a9678b4
3. mujoco helpful (debug) website: https://github.com/openai/mujoco-py/issues/410
4. https://www.reddit.com/r/Ubuntu/comments/rmz3mn/why_my_export_path_doesnt_work_mujoco_gcc_error/?rdt=40047
5. import typing, pandas, time, pickle, numpy, torch
6. install cuda : https://www.cherryservers.com/blog/install-cuda-ubuntu (self define the driver version) install cuDNN with following command :
conda install -c nvidia cuda-nvcc
conda install -c "nvidia/label/cuda-11.3.0" cuda-nvcc    #self define cuda version based on the detailed information given by:  nvidia-smi
7. run the script "FQE_load_two.py" directly.
8. The data will saved in FQE_load_2, remember to change the rate of saving Q-functions in the bottom of FQE_load_two, also change the saving picture name so it will not substitute original one
9. See pictures in k_figure


BVFT-PE-AVG:
load FQE Q-functions and do BVFT (trail, don't run, it's next step, finished before with wrong FQE policy)

Bvft_handle:
draw total reward of trained policy graph (single policy now)

BvftUtil: 
Bvft class, don't change 

Data_generation_policy_train: 
Train behavioral policy with ddpg

ddpg_greedy_model.d3: 
the saved behavioral policy ddpg

FQE_load_train: 
load dataset and generated policy, run FQE to generate Q-function 

Plot_util: 
Functions to draw picture, class of Bvft, functions to calculate k regret (not accurate now)

Policy_experiment_train:
train policy given behavioral data

trained_ddpg_policy_dataset_100000:
100000 replay buffer generated by ddpg (behavioral policy)
the saved dataset generated by behavioral policy

k_figure:
place to save figure

d3rlpy_logs:
automatic algorithm save place 

policy_saving_place:
the folder to save genreated policy 

FQE_saveplace_cql_1024_2e-5:
place to save FQE result q function for cql with hidden layer [128,1024] and learning rate 2e-5
